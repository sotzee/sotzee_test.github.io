# ADD MSC AND BSC THESISES. AND PHD

- title: "Customisable multitenant web form with JSF and MySQL"
  authors: Bazilinskyy, P.
  url: bazilinskyy2012customisable
  image: bazilinskyy2012customisable.jpg
  display: "Thesis for: BEng in Information Technology"
  year: 2012
  code: https://github.com/bazilinskyy/multitenant-webforms
  abstract: "There"

# - title: "Multi-core Insense"
#   authors: Bazilinskyy, P.
#   url: bazilinskyy2013multi
#   image: bazilinskyy2013multi.jpg
#   display: "Thesis for: Erasmus Mundus Double MSc in Dependable Software Systems"
#   year: 2013
#   code: https://github.com/bazilinskyy/multicore-insense-runtime
#   abstract: "This project set out to investigate the benefits of using private heaps for memory management and static thread placement for optimising performance and cache usage. For this study, Insense, which is a component-based programming language developed in the University of St Andrews that abstracts over complications of memory management, concurrency control and synchronisation, was used (Dearle et al. 2008). Two memory management schemes are under investigation: use of a single shared heap and use of multiple private heaps. Further, three thread placement schemes are designed and implemented: 1) even distribution among cores; 2) placing all components on a single core; 3) locating Insense components based on frequency of inter-component communication. Furthermore, several elements of this investigation are worth emphasizing. With regard to allocation and deallocation of memory taking place in component instances running on different cores, the efficiency of using a private heap for each component resulted in speedup by a factor of 16. Then, utilising private heaps reduces a number of L1 cache misses by ~30%. Distributing components over cores according to communication pattern, for the most part performed similar to allowing the OS to perform thread placement dynamically according to load balance. In cases where no exchange of data between components takes place, static placement outperformed because there is no computation which may make load balancing dynamic placement of threads under control of the OS difficult. In this case, the static placement scheme was faster than dynamic balancing by a factor of 2.4."

# - title: "Impact of cache on data-sharing in multi-threaded programmes"
#   authors: Bazilinskyy, P.
#   url: bazilinskyy2014impact
#   image: bazilinskyy2014impact.jpg
#   display: "Thesis for: Erasmus Mundus Double MSc in Dependable Software Systems"
#   year: 2014
#   code: https://github.com/bazilinskyy/cache-mt
#   abstract: "This thesis answers the question whether a scheduler needs to take into account where communicating threads in multi-threaded applications are executed. The impact of cache on data-sharing in multi-threaded environments is measured. This work investigates a common base–case scenario in the telecommunication industry, where a programme has one thread that writes data and one thread that reads data. A taxonomy of inter-thread communication is defined. Furthermore, a mathematical model that describes inter-thread communication is presented. Two cycle–level experiments were designed to measure latency of CPU registers, cache and main memory. These results were utilised to quantify the model. Three application–level experiments were used to verify the model by comparing predictions of the model and data received in the real-life setting. The model broadens the applicability of experimental results, and it describes three types of communication outlined in the taxonomy. Storing communicating data across all levels of cache does have an impact on the speed of data–intense multi-threaded applications. Scheduling threads in a sender–receiver scenario to different dies in a multi-chip processor decreases speed of execution of such programmes by up to 37%. Pinning such threads to different cores in the same chip results in up to 5% decrease in speed of execution. The findings of this study show how threads need to be scheduled by a cache-aware scheduler. This project extends the author’s previous work, which investigated cache interference."

# - title: "Auditory interfaces in automated driving: an international survey"
#   authors: Bazilinskyy, P., De Winter, J. C. F.
#   url: bazilinskyy2015auditory
#   image: bazilinskyy2015auditory.jpg
#   display: PeerJ Computer Science, 1, e13
#   year: 2015
#   suppmat: https://doi.org/10.7717/peerj-cs.13/supp-1
#   doi: https://doi.org/10.7717/peerj-cs.13
#   abstract: "This study investigated peoples’ opinion on auditory interfaces in contemporary cars and their willingness to be exposed to auditory feedback in automated driving. We used an Internet-based survey to collect 1,205 responses from 91 countries. The respondents stated their attitudes towards two existing auditory driver assistance systems, a parking assistant (PA) and a forward collision warning system (FCWS), as well as towards a futuristic augmented sound system (FS) proposed for fully automated driving. The respondents were positive towards the PA and FCWS, and rated the willingness to have automated versions of these systems as 3.87 and 3.77, respectively (on a scale from 1 = disagree strongly to 5 = agree strongly). The respondents tolerated the FS (the mean willingness to use it was 3.00 on the same scale). The results showed that among the available response options, the female voice was the most preferred feedback type for takeover requests in highly automated driving, regardless of whether the respondents’ country was English speaking or not. The present results could be useful for designers of automated vehicles and other stakeholders."

# - title: "An international crowdsourcing study into people’s statements on fully automated driving"
#   authors: Bazilinskyy, P., Kyriakidis, M., De Winter, J. C. F.
#   url: bazilinskyy2015international
#   image: bazilinskyy2015international.jpg
#   display: Proceedings of International Conference on Applied Human Factors and Ergonomics (AHFE). Las Vegas, USA
#   year: 2015
#   doi: 10.1016/j.promfg.2015.07.540
#   abstract: "Fully automated driving can potentially provide enormous benefits to society. However, it has been unclear whether people will appreciate such far-reaching technology. This study investigated anonymous textual comments regarding fully automated driving, based on data extracted from three online surveys with 8,862 respondents from 112 countries. Initial filtering of comments with fewer than 15 characters resulted in 1,952 comments. The sample consisted primarily of males (74%) and had a mean age of 32.6 years. Next, we launched a crowdsourcing job and asked 69 workers to assign each of the 1,952 comments to at least one of 12 predefined categories, which included positive and negative attitude to automated driving, enjoyment in manual driving, concerns about trust, reliability of software, and readiness of road infrastructure. 46% of the comments were classified into the category ‘no meaningful information about automated driving’, leaving 792 comments for further analysis. 39% of the comments were classified as ‘positive attitude towards automated driving’ and 23% were classified as ‘negative attitude towards automated driving’. In conclusion, the public opinion appears to be split, with a substantial number of respondents being positive and a significant number of respondents being negative towards fully automated driving."

# - title: "Report on the in-vehicle auditory interactions workshop: taxonomy, challenges, and approaches"
#   authors: Jeon, M., Bazilinskyy, P., Hammerschmidt, J., Hermann, T., Landry, S., Wolf, K. E.
#   url: jeon2015report
#   image: jeon2015report.jpg
#   display: Proceedings of AutomotiveUI. Graz, Austria
#   year: 2015
#   abstract: "As driving is mainly a visual task, auditory displays play a critical role for in-vehicle interactions.To improve in-vehicle auditory interactions to the advanced level, auditory display researchers and automotive user interface researchers came together to discuss this timely topic at an in-vehicle auditory interactions workshop at the International Conference on Auditory Display (ICAD).The present paper reports discussion outcomes from the workshop for more discussions at the AutoUI conference."

# - title: "Blind driving by means of auditory feedback"
#   authors: Bazilinskyy, P., Geest, L. Van Der, Van Leeuwen, S., Numan, B., Pijnacker, J., De Winter, J. C. F.
#   url: bazilinskyy2016blind
#   image: bazilinskyy2016blind.jpg
#   display: Proceedings of IFAC/IFIP/IFORS/IEA Symposium on Analysis, Design, and Evaluation of Human-Machine Systems. Kyoto, Japan
#   year: 2016
#   doi: 10.1016/j.ifacol.2016.10.612
#   suppmat: https://doi.org/10.4121/uuid:6c02218f-cd2c-4eb2-8c93-1eeac44690ed
#   abstract: "Driving is a safety-critical task that predominantly relies on vision. However, visual information from the environment is sometimes degraded or absent. In other cases, visual information is available, but the driver fails to use it due to distraction or impairment. Providing drivers with real-time auditory feedback about the state of the vehicle in relation to the environment may be an appropriate means of support when visual information is compromised. In this study, we explored whether driving can be performed solely by means of artificial auditory feedback. We focused on lane keeping, a task that is vital for safe driving. Three auditory parameter sets were tested: (1) predictor time, where the volume of a continuous tone was a linear function of the predicted lateral error from the lane centre 0 s, 1 s, 2 s, or 3 s into the future; (2) feedback mode (volume feedback vs. beep-frequency feedback) and mapping (linear vs. exponential relationship between predicted error and volume/beep frequency); and (3) corner support, in which in addition to volume feedback, a beep was offered upon entering/leaving a corner, or alternatively when crossing the lane centre while driving in a corner. A dead-zone was used, whereby the volume/beep-frequency feedback was provided only when the vehicle deviated more than 0.5 m from the centre of the lane. An experiment was conducted in which participants (N = 2) steered along a track with sharp 90-degree corners in a simulator with the visual projection shut down. Results showed that without predictor feedback (i.e., 0 s prediction), participants were more likely to depart the road compared to with predictor feedback. Moreover, volume feedback resulted in fewer road departures than beep-frequency feedback. The results of this study may be used in the design of in-vehicle auditory displays. Specifically, we recommend that feedback be based on anticipated error rather than current error."

# - title: "Sonifying the location of an object: A comparison of three methods"
#   authors: Bazilinskyy, P., Van Haarlem, W., Quraishi, H., Berssenbrugge, C., Binda, J., De Winter, J. C. F.
#   url: bazilinskyy2016sonifying
#   image: bazilinskyy2016sonifying.jpg
#   display: Proceedings of IFAC/IFIP/IFORS/IEA Symposium on Analysis, Design, and Evaluation of Human-Machine Systems. Kyoto, Japan
#   year: 2016
#   doi: 10.1016/j.ifacol.2016.10.614
#   abstract: "Auditory displays are promising for informing operators about hazards or objects in the environment. However, it remains to be investigated how to map distance information to a sound dimension. In this research, three sonification approaches were tested: Beep Repetition Rate (BRR) in which beep time and inter-beep time were a linear function of distance, Sound Intensity (SI) in which the digital sound volume was a linear function of distance, and Sound Fundamental Frequency (SFF) in which the sound frequency was a linear function of distance. Participants (N = 29) were presented with a sound by means of headphones and subsequently clicked on the screen to estimate the distance to the object with respect to the bottom of the screen (Experiment 1), or the distance and azimuth angle to the object (Experiment 2). The azimuth angle in Experiment 2 was sonified by the volume difference between the left and right ears. In an additional Experiment 3, reaction times to directional audio-visual feedback were compared with directional visual feedback. Participants performed three sessions (BRR, SI, SFF) in Experiments 1 and 2 and two sessions (visual, audio-visual) in Experiment 3, 10 trials per session. After each trial, participants received knowledge-of-results feedback. The results showed that the three proposed methods yielded an overall similar mean absolute distance error, but in Experiment 2 the error for BRR was significantly smaller than for SI. The mean absolute distance errors were significantly greater in Experiment 2 than in Experiment 1. In Experiment 3, there was no statistically significant difference in reaction time between the visual and audio-visual conditions. The results are interpreted in light of the Weber-Fechner law, and suggest that humans have the ability to accurately interpret artificial sounds on an artificial distance scale."

# - title: "Object-alignment performance in a head-mounted display versus a monitor"
#   authors: Bazilinskyy, P., Kovácsová, N., Al Jawahiri, A., Kapel, P., Mulckhuyse, J., Wagenaar, S., De Winter, J. C. F.
#   url: bazilinskyy2016object
#   image: bazilinskyy2016object.jpg
#   display: Proceedings of IEEE International Conference on Systems, Man, and Cybernetics (SMC). Budapest, Hungary
#   year: 2016
#   abstract: "Head-mounted displays (HMDs) offer immersion and binocular disparity. This study investigated whether an HMD yields better object-alignment performance than a conventional monitor in virtual environments that are rich in pictorial depth cues. To determine the effects of immersion and disparity separately, three hardware setups were compared: (1) a conventional computer monitor, yielding low immersion, (2) an HMD with binocular-vision settings (HMD stereo), and (3) an HMD with the same image presented to both eyes (HMD mono). Two virtual environments were used: a street environment in which two cars had to be aligned (target distance of about 15 m) and an office environment in which two books had to be aligned (target distance of about 0.7 m, at which binocular depth cues were expected to be important). Twenty males (mean age = 21.2, SD age = 1.6) each completed 10 object-alignment trials for each of the six conditions. The results revealed no statistically significant differences in object-alignment performance between the three hardware setups. A self-report questionnaire showed that participants felt more involved in the virtual environment and experienced more oculomotor discomfort with the HMD than with the monitor."

# - title: "Usefulness and satisfaction of take-over requests for highly automated driving"
#   authors: Bazilinskyy, P., Eriksson, A., Petermeijer, S. M., De Winter, J. C. F.
#   url: bazilinskyy2017usefulness
#   image: bazilinskyy2017usefulness.jpg
#   display: Proceedings of Road Safety and Simulation (RSS). The Hague, The Netherlands
#   year: 2017
#   suppmat: https://www.dropbox.com/sh/sb2180f8t27hw3x/AAAMIuifV7NlVv6T3xqpXW8ja
#   abstract: "This paper summarizes our results from survey research and driving simulator experiments on auditory, vibrotactile, and visual take-over requests in highly automated driving. Our review shows that vibrotactile takeover requests in the driver’s seat yielded relatively high ratings of self-reported usefulness and satisfaction. Auditory take-over requests in the form of beeping sound were regarded as useful but not satisfactory, and it was found that the beep rate corresponds to perceived urgency. Visual-only feedback (LEDs) was regarded by participants as neither useful nor satisfactory. Augmented visual feedback was found to support effective steering and braking actions, and may be a useful compliment to vibrotactile take-over requests. The present findings may be used in the design of take-over requests."

# - title: "Blind driving by means of a steering-based predictor algorithm"
#   authors: Bazilinskyy, P., Beaumont, C. J. A. M., Van der Geest, X. O. S., De Jonge, R. F., Van der Kroft, K., De Winter, J. C. F. 
#   url: bazilinskyy2017blind
#   image: bazilinskyy2017blind.jpg
#   display: Proceedings of International Conference on Applied Human Factors and Ergonomics (AHFE) Los Angeles, USA
#   year: 2017
#   suppmat: https://doi.org/10.4121/uuid:6c02218f-cd2c-4eb2-8c93-1eeac44690ed
#   abstract: "The aim of this work was to develop and empirically test different algorithms of a lane-keeping assistance system that supports drivers by means of a tone when the car is about to deviate from its lane. These auditory assistance systems were tested in a driving simulator with its screens shut down, so that the participants used auditory feedback only. Five participants drove with a previously published algorithm that predicted the future position of the car based on the current velocity vector, and three new algorithms that predicted the future position based on the momentary speed and steering angle. Results of a total of 5 hours of driving across participants showed that, with extensive practice and knowledge of the system, it is possible to drive on a track with sharp curves for 5 minutes without leaving the road. Future research should aim to improve the intuitiveness of the auditory feedback."

# - title: "Take-over again: Investigating multimodal and directional TORs to get the driver back into the loop"
#   authors: Petermeijer, S. M., Bazilinskyy, P.*, Bengler, K., De Winter, J. C. F.
#   url: petermeijer2017take
#   image: petermeijer2017take.jpg
#   display: Applied Ergonomics, 62, 204–215
#   year: 2017
#   doi: 10.1016/j.apergo.2017.02.023
#   suppmat: https://doi.org/10.1016/j.apergo.2017.02.023
#   abstract: "When a highly automated car reaches its operational limits, it needs to provide a takeover request (TOR) in order for the driver to resume control. The aim of this simulator-based study was to investigate the effects of TOR modality and left/right directionality on drivers' steering behaviour when facing a head-on collision without having received specific instructions regarding the directional nature of the TORs. Twenty-four participants drove three sessions in a highly automated car, each session with a different TOR modality (auditory, vibrotactile, and auditory-vibrotactile). Six TORs were provided per session, warning the participants about a stationary vehicle that had to be avoided by changing lane left or right. Two TORs were issued from the left, two from the right, and two from both the left and the right (i.e., nondirectional). The auditory stimuli were presented via speakers in the simulator (left, right, or both), and the vibrotactile stimuli via a tactile seat (with tactors activated at the left side, right side, or both). The results showed that the multimodal TORs yielded statistically significantly faster steer-touch times than the unimodal vibrotactile TOR, while no statistically significant differences were observed for brake times and lane change times. The unimodal auditory TOR yielded relatively low self-reported usefulness and satisfaction ratings. Almost all drivers overtook the stationary vehicle on the left regardless of the directionality of the TOR, and a post-experiment questionnaire revealed that most participants had not realized that some of the TORs were directional. We conclude that between the three TOR modalities tested, the multimodal approach is preferred. Moreover, our results show that directional auditory and vibrotactile stimuli do not evoke a directional response in uninstructed drivers. More salient and semantically congruent cues, as well as explicit instructions, may be needed to guide a driver into a specific direction during a takeover scenario."

# - title: "Analyzing crowdsourced ratings of speech-based take-over requests for automated driving"
#   authors: Bazilinskyy, P., De Winter, J. C. F.
#   url: bazilinskyy2017analyzing
#   image: bazilinskyy2017analyzing.jpg
#   display: Applied Ergonomics, 64, 56–64
#   year: 2017
#   doi: 10.1016/j.apergo.2017.05.001
#   suppmat: https://doi.org/10.1016/j.apergo.2017.05.001
#   abstract: "Take-over requests in automated driving should fit the urgency of the traffic situation. The robustness of various published research findings on the valuations of speech-based warning messages is unclear. This research aimed to establish how people value speech-based take-over requests as a function of speech rate, background noise, spoken phrase, and speaker’s gender and emotional tone. By means of crowdsourcing, 2,669 participants from 95 countries listened to a random 10 out of 140 take-over requests, and rated each take-over request on urgency, commandingness, pleasantness, and ease of understanding. Our results replicate several published findings, in particular that an increase in speech rate results in a monotonic increase of perceived urgency and commandingness. The female voice was preferred over a male voice when there was a high level of background noise, a finding that contradicts the literature. Moreover, a take-over request spoken with Indian accent was found to be easier to understand by participants from India compared to participants from other countries. Our results replicate effects in the literature regarding speech-based warnings, and shed new light on effects regarding background noise, gender, and nationality. The results may have implications for the selection of appropriate take-over requests in automated driving. Additionally, our study demonstrates the promise of crowdsourcing for testing human factors and ergonomics theories with large sample sizes."

# - title: "An auditory dataset of passing vehicles recorded with a smartphone"
#   authors: Bazilinskyy, P., Van der Aa, A., Schoustra, M., Spruit, J., Staats, L., Van der Vlist, K. J., De Winter, J. C. F.
#   url: bazilinskyy2018auditory
#   image: bazilinskyy2018auditory.jpg
#   display: Proceedings of Tools and Methods of Competitive Engineering (TMCE). Las Palmas de Gran Canaria, Spain
#   year: 2018
#   suppmat: http://doi.org/10.4121/uuid:bef54ab8-73ef-42f3-b6b7-54e011737e72
#   abstract: "The increase of smartphones over the past decade has contributed to distraction in traffic. However, smartphones could potentially be turned into an advantage by being able to detect whether a motorized vehicle is passing the smartphone user (e.g., a pedestrian or cyclist). Herein, we present a dataset of audio recordings of passing vehicles, made with a smartphone. Recordings were made of a passing passenger car and a scooter in various conditions (windy weather vs. calm weather, approaching from the front vs. from behind, 1 m, 2 m, and 3 m distance between smartphone and vehicle, vehicle driving with 30 vs. 50 km/h, and smartphone being stationary vs. moving with the cyclist). Data from an 8-microphone array, video recordings, and GPS data of vehicle position and speed are provided as well. Our present dataset may prove useful in the development of mobile apps that detect a passing motorized vehicle, or for transportation research."

# - title: "Eye movements while cycling in GTA V"
#   authors: Bazilinskyy, P., Heisterkamp, N., Luik, P., Klevering, S., Haddou, A., Zult, M., Dialynas, G., Dodou, D., De Winter, J. C. F.
#   url: bazilinskyy2018eye
#   image: bazilinskyy2018eye.jpg
#   display: Proceedings of Tools and Methods of Competitive Engineering (TMCE). Las Palmas de Gran Canaria, Spain
#   year: 2018
#   code: https://github.com/bazilinskyy/gtav-sim
#   abstract: "A common limitation in human factors research is that vehicle simulators often lack perceptual fidelity. Video games, on the other hand, are becoming increasingly realistic and may be a promising tool for simulator-based human factors research. In this work, we explored whether an off-the-shelf video game is suitable for research purposes. We used Grand Theft Auto (GTA) V combined with a Smart Eye DR120 eye tracker to measure eye movements of participants cycling in hazardous traffic situations. Twenty-seven participants encountered various situations that are representative of urban cycling, such as intersection crossings, a car leaving a parking spot in front of the cyclist, and the opening of a car door in front of the cyclist. Data of participants' gaze on the computer monitor as recorded by the eye tracker were translated into 3D coordinates in the virtual world, as well as into semantic information regarding the objects that the participant was focusing on. We conclude that GTA V allows for the collection of useful data for human factors research."

# - title: "Take-over requests in highly automated driving: A crowdsourcing survey on auditory, vibrotactile, and visual displays"
#   authors: Bazilinskyy, P., Petermeijer, S. M., Petrovych, V., Dodou, D., De Winter, J. C. F.
#   url: bazilinskyy2018take
#   image: bazilinskyy2018take.jpg
#   display: "Transportation Research Part F: Traffic Psychology and Behaviour, 56, 82–98"
#   year: 2018
#   doi: 10.1016/j.trf.2018.04.001
#   suppmat: https://doi.org/10.4121/uuid:e3908ec5-d086-4737-8d4a-d4046dbbc53c
#   abstract: "An important research question in the domain of highly automated driving is how to aid drivers in transitions between manual and automated control. Until highly automated cars are available, knowledge on this topic has to be obtained via simulators and self-report questionnaires. Using crowdsourcing, we surveyed 1692 people on auditory, visual, and vibrotactile take-over requests (TORs) in highly automated driving. The survey presented recordings of auditory messages and illustrations of visual and vibrational messages in traffic scenarios of various urgency levels. Multimodal TORs were the most preferred option in high-urgency scenarios. Auditory TORs were the most preferred option in low-urgency scenarios and as a confirmation message that the system is ready to switch from manual to automated mode. For low-urgency scenarios, visual-only TORs were more preferred than vibration-only TORs. Beeps with shorter interpulse intervals were perceived as more urgent, with Stevens’ power law yielding an accurate fit to the data. Spoken messages were more accepted than abstract sounds, and the female voice was more preferred than the male voice. Preferences and perceived urgency ratings were similar in middle- and high-income countries. In summary, this international survey showed that people’s preferences for TOR types in highly automated driving depend on the urgency of the situation."

# - title: "Crowdsourced measurement of reaction times to audiovisual stimuli with various degrees of asynchrony"
#   authors: Bazilinskyy, P., De Winter, J. C. F.
#   url: bazilinskyy2018crowdsourced
#   image: bazilinskyy2018crowdsourced.jpg
#   display: Human Factors, 60, 1192–120
#   year: 2018
#   doi: 10.1177/0018720818787126
#   code: https://github.com/bazilinskyy/auditory-tor-rt
#   suppmat: https://doi.org/10.4121/uuid:673c9bbc-bf17-42fa-a23a-3d716e141b1f
#   abstract: "Objective: This study aimed to replicate past research concerning reaction times to audiovisual stimuli with different stimulus onset asynchrony (SOA), using a large sample of crowdsourcing respondents. Background: Research has shown that reaction times are fastest when an auditory and a visual stimulus are presented simultaneously and that SOA causes an increase in reaction time, this increase being dependent on stimulus intensity. Research on audiovisual SOA has been conducted with small numbers of participants. Method: 1,823 participants each performed 176 reaction time trials consisting of 29 SOA levels and three visual intensity levels, using CrowdFlower, with a compensation of USD 0.20 per participant. Results were verified with a local web-in-lab study (N = 34). Results: The results replicated past research, with a V-shape of mean reaction time as a function of SOA, the V-shape being stronger for lower intensity visual stimuli. The level of SOA affected mainly the right side of the reaction time distribution, whereas the fastest 5% were hardly affected. The variability of reaction times was higher for the crowdsourcing study than for the web-in-lab study. Conclusion: Crowdsourcing is a promising medium for reaction-time research that involves small temporal differences in stimulus presentation. The observed effects of SOA can be explained by an independent-channels mechanism, and also by some participants not perceiving the auditory or visual stimulus, hardware variability, misinterpretation of the task instructions, or lapses in attention. Application: The obtained knowledge on the distribution of reaction times may benefit the design of warning systems."

# - title: "Auditory interface for automated driving"
#   authors: Bazilinskyy, P.
#   url: bazilinskyy2018auditoryinterface
#   image: bazilinskyy2018auditoryinterface.jpg
#   display: PhD thesis
#   year: 2018
#   abstract: "Automated driving may be a key to solving a number of problems that humanity faces today: large numbers of fatalities in traffic, traffic congestions, and increased gas emissions. However, unless the car drives itself fully automatically (such a car would not need to have a steering wheel, nor accelerator and brake pedals), the driver needs to receive information from the vehicle. Such information can be delivered by sound, visual displays, vibrotactile feedback, or a combination of two or three kinds of signals. Sound may be a particularly promising feedback modality, as sound can attract a driver’s attention irrespective of his/her momentary visual attention.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Although ample research exists on warning systems and other types of auditory displays, what is less well known is how to design warning systems for automated driving specifically.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Taking over control from an automated car is a spatially demanding task that may involve a high level of urgency, and warning signals (also called ‘take- over requests’, TORs) need to be designed so that the driver reacts as quickly and safely as possible. Furthermore, little knowledge is available on how to support the situation awareness and mode awareness of drivers of automated cars. The goal of this thesis is to discover how the auditory modality should be used during automated driving and to contribute towards the development of design guidelines.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;First, this thesis describes the state-of-the-art (Chapter 2) by examining and improving the current sound design process in the industry, and by examining the requirements of the future users of automated cars, the public (Chapter 2). Next, the thesis focuses on the design of discrete warnings/TORs (Chapter 3), the use of sound for supporting situation awareness (Chapter 4), and mode awareness (Chapter 5). Finally, Chapters 6 and 7 provide a future outlook, conclusions, and recommendations. The content of the thesis is described in more detail below.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Chapter 2 describes state of the art in the domain of the use of sound in the automotive industry. Section 2.1 presents a new sound design process for the automotive industry developed with Continental AG, consisting of 3 stages: description, design/creation, and verification. An evaluation of the process showed that it supports the more efficient creation of auditory assets than the unstructured process that was previously employed in the company.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;To design good feedback is not enough, it also needs to be appreciated by users. To this end, Section 2.2 describes a crowdsourced online survey that was used to investigate peoples’ opinion of 1,205 responses from 91 countries on auditory interfaces in modern cars and their readiness to have auditory feedback in automated vehicles. The study was continued in another crowdsourced online survey described in Section 2.3, where 1,692 people were surveyed on auditory, visual, and vibrotactile TORs in scenarios of varying levels of urgency. Based on the results, multimodal TORs were the most preferred option in scenarios associated with high urgency. Sound-based TORs were the most favored choice in scenarios with low urgency. Auditory feedback was also preferred for confirmation that the system is ready to switch from manual to automated mode. Speech-based feedback was more accepted than artificial sounds, and the female voice was more preferred than the male voice as a take-over request.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;To understand better how sound may be used during fully automated driving, it is crucial to acknowledge the opinion of potential end users of such vehicles on the technology. Section 2.4 investigates anonymous textual comments concerning fully automated driving by using data from three Internet- based surveys (including the surveys described in Sections 2.2 and 2.3) with 8,862 respondents from 112 countries. The opinion was split: 39% of the comments were positive towards automated driving and 23% were seen as such that express negative attitude towards automated driving.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Chapter 3 focuses on the use of the auditory modality to support TORs. Section 3.1 describes a crowdsourcing experiment on reaction times to audiovisual stimuli with different stimulus onset asynchrony (SOA). 1,823 participants each performed 176 reaction time trials consisting of 29 SOA levels and three visual intensity levels. The results replicated past research, with a V- shape of mean reaction time as a function of SOA. The study underlines the power of crowdsourced research, and shows that auditory and visual warnings need to be provided at exactly the same moment in order to generate optimally fast response times. The results also indicate large individual differences in reaction times to different SOA levels, a finding which implicates that multimodal feedback has important advantages as compared to unimodal feedback.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Then, in Section 3.2 focus was given to speech-based TORs. In a crowdsourced study, 2,669 participants from 95 countries listened to a random 10 out of 140 TORs, and rated each TOR on ease of understanding, pleasantness, urgency, and commandingness. Increased speech rate results in an increase of perceived urgency and commandingness. With high level of background noise, the female voice was preferred over the male voice, which contradicts the literature. Furthermore, a take-over request spoken by a person with Indian accent was easier to understand by participants from India compared to participants from other countries.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The results of the studies in Chapter 2 and Sections 3.1 and 3.2 were used to design a simulator-based study presented in Section 3.3. 24 participants took part in three sessions in a highly automated car (different TOR modality in each session: auditory, vibrotactile, and auditory-vibrotactile). TORs were played from the right, from the left, and from both left and right. The auditory TOR yielded comparatively low ratings of usefulness and satisfaction. Regardless of the directionality of the TOR, almost all drivers overtook the stationary vehicle on the left.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Section 3.4 summarizes results from survey research (Sections 2.2, 2.3, 3.1, 3.2) and driving simulator experiments (including Section 3.3) on TORs executed with one or multiple of the three modalities. Results showed that vibrotactile TORs in the driver’s seat yielded relatively high ratings of self- reported usefulness and satisfaction. Auditory TORs in the form of beeps were regarded as useful but not satisfactory, and it was found that an increase of beep rate yields an increase of self-reported urgency. Visual-only feedback in the form of LEDs was seen by participants as neither useful nor satisfactory.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Chapter 4 draws attention to the use of auditory feedback for the situation awareness during manual and automated driving. Section 4.1 investigates how to represent distance information by means of sound. Three sonification approaches were tested: Beep Repetition Rate, Sound Intensity, and Sound Fundamental Frequency. The three proposed methods produced a similar mean absolute distance error.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;These results were used in three simulator-based experiments (Sections 4.2–4.4) to examine the idea whether it is possible to drive a car blindfolded with the use of continuous auditory feedback only. Different types of sonification (e.g., volume-based, beep-frequency based) were used, and the auditory feedback was provided when deviating more than 0.5 m from lane center. In all experiments, people drove on a track with sharp 90-degree corners while speed control was automated. Results showed no clear effects of sonification method on lane-keepign performance, but it was found that it is vital to not give feedback based on the current lateral position, but based on where the car will be about 2 seconds into the future. The predictor algorithm should consider the velocity vector of the car as well as the momentary steering wheel angle. Results showed that, with extensive practice and knowledge of the system, it is possible to drive on a track for 5 minutes without leaving the road. Drivers benefit from simple auditory feedback and additional stimuli add workload without improving performance.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Chapter 5 examines the use of sound for mode awareness during highly automated driving. An on-road experiment in a heavy truck equipped with low- level automated is described. I used continuous auditory feedback on the status of ACC, lane offset, and headway, which blends with the engine and wind sounds that are already present in the cabin. 23 truck drivers were presented with the additional sounds in isolation and in combination. Results showed that the sounds were easy to understand and that the lane-offset sound was regarded as somewhat useful. However, participants overall preferred a silent cabin and expressed displeasure with the idea of being presented with extra sounds on a continuous basis.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Chapter 6 provides an outlook on when fully automated driving may become a reality. In 12 crowdsourcing studies conducted between 2014 and 2017 (including the studies described in Sections 2.2, 2.3, 3.1, 3.2), 17,360 people from 129 countries were asked when they think that most cars will be able to drive fully automatically in their country of residence. The median reported year was 2030. Over the course of three years respondents have moderated their expectations regarding the penetration of fully automated cars. The respondents appear to be more optimistic than experts.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Chapter 7 presents a discussion and conclusions derived from all chapters in the thesis.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;• The most preferred way to support a TOR is an auditory instruction in the form of a female voice.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;• The preferences of people depend on the urgency of the situation.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;• Reaction times are fastest when an auditory and a visual stimulus are presented at the same moment rather than with a temporal asynchrony.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;• An increase of beep rate yields an increase of self-reported urgency.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;• An increase in the speech rate results in an increase of perceived urgency and commandingness.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;• If the goal is for drivers to react as quickly as possible, multimodal feedback should be used.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;• It is important to use a preview controller (look-ahead time) for supporting drivers’ situation awareness in a lane keeping task.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;• Truck drivers are not favorable towards adding additional continuous feedback to the cabin, even though the feedback is easy to understand.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In summary, in this thesis I evaluated the use of sound as discrete warnings, but also as a means of continuous/spatial support for situation/mode awareness."

# - title: "Sound design process for automotive industry"
#   authors: Bazilinskyy, P., Cieler, S., De Winter, J. C. F.
#   url: bazilinskyy2018sound
#   image: bazilinskyy2018sound.jpg
#   display: Preprint
#   year: 2018
#   code: https://github.com/bazilinskyy/wordsforsound
#   suppmat: https://www.dropbox.com/sh/u41zopx1220x97x/AAAr3j0abXrYOeezfKWut5F1a
#   abstract: "The automotive industry is recognised as a challenging arena for sound design, because the presented information needs to not only comply with safety regulations but also match subjective expectations. By means of a structured interview with ten employees (software developers, quality analysists, sound designers, engineers, managers) of the company Continental, we collected requirements for the sound design process in an automotive industry setting. We present a sound design process consisting of three stages: description, design/creation, and verification. We developed a prototype of a web-based application to support the process."

# - title: "Continuous auditory feedback on the status of adaptive cruise control, lane deviation, and time headway: An acceptable support for truck drivers?"
#   authors: Bazilinskyy, P., Larsson, P., Johansson, E., De Winter, J. C. F.
#   url: bazilinskyy2019continuous
#   image: bazilinskyy2019continuous.jpg
#   display: Acoustical Science and Technology, 40, 382–390
#   year: 2019
#   doi: 10.1250/ast.40.382
#   code: https://github.com/bazilinskyy/ambient-had-status
#   suppmat: https://doi.org/10.4121/uuid:6f781c05-c696-4d0e-9fc6-bbfb7247a19e
#   abstract: "The number of trucks that are equipped with driver assistance systems is increasing. These driver assistance systems typically offer binary auditory warnings or notifications upon lane departure, close headway, or automation (de)activation. Such binary sounds may annoy the driver if presented frequently. Truck drivers are well accustomed to the sound of the engine and wind in the cabin. Based on the premise that continuous sounds are more natural than binary warnings, we propose continuous auditory feedback on the status of adaptive cruise control, lane offset, and headway, which blends with the engine and wind sounds that are already present in the cabin. An on-road study with 23 truck drivers was performed, where participants were presented with the additional sounds in isolation from each other and in combination. Results showed that the sounds were easy to understand and that the lane-offset sound was regarded as somewhat useful. Systems with feedback on the status of adaptive cruise control and headway were seen as not useful. Participants overall preferred a silent cabin and expressed displeasure with the idea of being presented with extra sounds on a continuous basis. Suggestions are provided for designing less intrusive continuous auditory feedback."

# - title: "Blind driving by means of the track angle error"
#   authors: Bazilinskyy, P., Bijker, L., Dielissen, T., French, S., Mooijman, T., Peters, L., De Winter, J. C. F.
#   url: bazilinskyy2019blind
#   image: bazilinskyy2019blind.jpg
#   display: Proceedings of International Congress on Sound and Vibration (ICSV). Montreal, Canada
#   year: 2019
#   suppmat: https://doi.org/10.4121/uuid:6c02218f-cd2c-4eb2-8c93-1eeac44690ed
#   abstract: "This study is the third iteration in a series of studies aimed to develop a system that allows driving blindfolded. We used a sonification approach, where the predicted angular error of the car 2 seconds into the future was translated into spatialized beeping sounds. In a driving simulator experiment, we tested with 20 participants whether a surround-sound feedback system that uses four speakers yields better lane-keeping performance than binary directional feedback produced by two speakers. We also examined whether adding a corner support system to the binary system improves lane-keeping performance. Compared to the two previous iterations, this study presents a more realistic experimental setting, as participants were unfamiliar with the feedback system and received the feedback without headphones. The results show that participants had poor lane-keeping performance. Furthermore, the driving task was perceived as demanding, especially in the case of the additional corner support. Our findings from the blind driving projects suggest that drivers benefit from simple auditory feedback; additional auditory stimuli (e.g., corner support) add workload without improving performance."

# - title: "When will most cars drive fully automatically? An analysis of international surveys"
#   authors: Bazilinskyy, P., Kyriakidis, M., De Winter, J. C. F.
#   url: bazilinskyy2019will
#   image: bazilinskyy2019will.jpg
#   display: "Transportation Research Part F: Traffic Psychology and Behaviour, 64, 184-195"
#   year: 2019
#   doi: 10.1016/j.trf.2019.05.008
#   suppmat: http://doi.org/10.4121/uuid:ed63e704-ac75-4f96-a2d7-4c8e3b48b168
#   abstract: "When fully automated cars will be widespread is a question that has attracted considerable attention from futurists, car manufacturers, and academics. This paper aims to poll the public’s expectations regarding the deployment of fully automated cars. In 15 crowdsourcing surveys conducted between June 2014 and January 2019, we obtained answers from 18,970 people in 128 countries regarding when they think that most cars will be able to drive fully automatically in their country of residence. The median reported year was 2030. The later the survey date, the smaller the percentage of respondents who reported that most cars would be able to drive fully automatically by 2020, with 15–22% of the respondents providing this estimate in the surveys conducted between 2014 and 2016 versus 3–5% in the 2018 surveys. Respondents who completed multiple surveys were more likely to revise their estimate upward (39.4%) than downward (35.3%). Correlational analyses showed that people from more affluent countries and people who have heard of the Google Driverless Car (Waymo) or the Tesla Autopilot reported a significantly earlier year. Finally, we made a comparison between the crowdsourced respondents and respondents from a technical university who answered the same question; the median year reported by the latter group was 2040. We conclude that over the course of 4.5 years the public has moderated its expectations regarding the penetration of fully automated cars but remains optimistic compared to what experts currently believe."

# - title: "Survey on eHMI concepts: The effect of text, color, and perspective"
#   authors: Bazilinskyy, P., Dodou, D., De Winter, J. C. F.
#   url: bazilinskyy2019survey
#   image: bazilinskyy2019survey.jpg
#   display: "Transportation Research Part F: Traffic Psychology and Behaviour, 67, 175-194"
#   year: 2019
#   doi: 10.1016/j.trf.2019.10.013
#   suppmat: https://doi.org/10.4121/uuid:9ea78136-3ffc-4194-84f7-3116b6a55758
#   abstract: "The automotive industry has presented a variety of external human-machine interfaces (eHMIs) for automated vehicles (AVs). However, there appears to be no consensus on which types of eHMIs are clear to vulnerable road users. Here, we present the results of two large crowdsourcing surveys on this topic. In the first survey, we asked respondents about the clarity of 28 images, videos, and patent drawings of eHMI concepts presented by the automotive industry. Results showed that textual eHMIs were generally regarded as the clearest. Among the non-textual eHMIs, a projected zebra crossing was regarded as clear, whereas light-based eHMIs were seen as relatively unclear. A considerable proportion of the respondents mistook non-textual eHMIs for a sensor. In the second survey, we examined the effect of perspective of the textual message (egocentric from the pedestrian's point of view: 'Walk', 'Don't walk' vs. allocentric: 'Will stop', 'Won't stop') and color (green, red, white) on whether respondents felt safe to cross in front of the AV. The results showed that textual eHMIs were more persuasive than color-only eHMIs, which is in line with the results from the first survey. The eHMI that received the highest percentage of 'Yes' responses was the message 'Walk' in green font, which points towards an egocentric perspective taken by the pedestrian. We conclude that textual egocentric eHMIs are regarded as clearest, which poses a dilemma because textual instructions are associated with practical issues of liability, legibility, and technical feasibility."

# - title: "Coupled simulator for research on the interaction between pedestrians and (automated) vehicles"
#   authors: Bazilinskyy, P., Kooijman, L.*, De Winter, J. C. F.
#   url: bazilinskyy2020coupled
#   image: bazilinskyy2020coupled.jpg
#   display: Proceedings of Driving Simulation Conference (DSC). Antibes, France
#   year: 2020
#   suppmat: http://doi.org/10.4121/uuid:63072181-e9fb-4221-91e6-ba836ae3190c
#   code: https://github.com/bazilinskyy/coupled-sim
#   abstract: "Driving simulators are regarded as valuable tools for human factors research on automated driving and traffic safety. However, simulators that enable the study of human-human interactions are rare. In this study, we present an open-source coupled simulator developed in Unity. The simulator supports input from head-mounted displays, motion suits, and game controllers. It facilitates research on interactions between pedestrians and humans inside manual and automated vehicles. We present results of a demo experiment on the interaction between a passenger in an automated car equipped with an external human-machine interface, a driver of a manual car, and a pedestrian. We conclude that the newly developed open-source coupled simulator is a promising tool for future human factors research."

# - title: "External Human-Machine Interfaces: Which of 729 colors is best for signaling ‘Please (do not) Cross’?"
#   authors: Bazilinskyy, P., Dodou, D., De Winter, J. C. F.
#   url: bazilinskyy2020external
#   image: bazilinskyy2020external.jpg
#   display: Proceedings of IEEE International Conference on Systems, Man, and Cybernetics (SMC). Toronto, Canada
#   year: 2020
#   doi: 10.1109/SMC42975.2020.9282998
#   suppmat: https://doi.org/10.4121/12948650
#   abstract: "Future automated vehicles may be equipped with external human-machine interfaces (eHMIs) capable of signaling to pedestrians whether or not they can cross the road. There is currently no consensus on the correct colors for eHMIs. Industry and academia have already proposed a variety of eHMI colors, including red and green, as well as colors that are said to be neutral, such as cyan. A confusion that can arise with red and green is whether the color refers to the pedestrian (egocentric perspective) or the automated vehicle (allocentric perspective). We conducted two crowdsourcing experiments (N = 2000 each) with images depicting an automated vehicle equipped with an eHMI in the form of a rectangular display on the front bumper. The eHMI had one out of 729 colors from the RGB spectrum. In Experiment 1, participants rated the intuitiveness of a random subset of 100 of these eHMIs for signaling 'please cross the road', and in Experiment 2 for 'please do NOT cross the road'. The results showed that for 'please cross', bright green colors were considered the most intuitive. For 'please do NOT cross', red colors were rated as the most intuitive, but with high standard deviations among participants. In addition, some participants rated green colors as intuitive for 'please do NOT cross'. Results were consistent for men and women and for colorblind and non-colorblind persons. It is concluded that eHMIs should be green if the eHMI is intended to signal 'please cross', but green and red should be avoided if the eHMI is intended to signal 'please do NOT cross'. Various neutral colors can be used for that purpose, including cyan, yellow, and purple."

# - title: "Risk perception: A study using dashcam videos and participants from different world regions."
#   authors: Bazilinskyy, P., Eisma, Y. B., Dodou, D., De Winter, J. C. F. 
#   url: bazilinskyy2020risk
#   image: bazilinskyy2020risk.jpg
#   display: Traffic Injury Prevention, 21, 347–353
#   year: 2020
#   doi: 10.1080/15389588.2020.1762871
#   suppmat: https://doi.org/10.4121/uuid:cd649413-c707-4469-8c47-2e20a0ee1f87
#   abstract: "Objective: Research has shown that perceived risk is a vital variable in the understanding of road traffic safety. Having experience in a particular traffic environment can be expected to affect perceived risk. More specifically, drivers may readily recognize traffic hazards when driving in their own world region, resulting in high perceived risk (the expertise hypothesis). Oppositely, drivers may be desensitized to traffic hazards that are common in their own world region, resulting in low perceived risk (the desensitization hypothesis). This study investigated whether participants experienced higher or lower perceived risk for traffic situations from their region compared to traffic situations from other regions. Methods: In a crowdsourcing experiment, participants viewed dashcam videos from four regions: India, Venezuela, United States, and Western Europe. Participants had to press a key when they felt the situation was risky. Results: Data were obtained from 800 participants, with 52 participants from India, 75 from Venezuela, 79 from the United States, 32 from Western Europe, and 562 from other countries. The results provide support for the desensitization hypothesis. For example, participants from India perceived low risk for hazards (e.g., a stationary car on the highway) that were perceived as risky by participants from other regions. At the same time, support for the expertise hypothesis was obtained, as participants in some cases detected hazards that were specific to their own region (e.g., participants from Venezuela detected inconspicuous roadworks in a Venezuelan city better than did participants from other regions). Conclusion: We found support for the desensitization hypothesis and the expertise hypothesis. These findings have implications for cross-cultural hazard perception research."

# - title: "What driving style makes pedestrians think a passing vehicle is driving automatically?"
#   authors: Bazilinskyy, P., Sakuma, T., De Winter, J. C. F.
#   url: bazilinskyy2021driving
#   image: bazilinskyy2021driving.jpg
#   display: Applied Ergonomics, 95, 103428
#   year: 2021
#   doi: 10.1016/j.apergo.2021.103428
#   suppmat: https://doi.org/10.4121/14077352
#   abstract: "An important question in the development of automated vehicles (AVs) is which driving style AVs should adopt and how other road users perceive them. The current study aimed to determine which AV behaviours contribute to pedestrians’ judgements as to whether the vehicle is driving manually or automatically as well as judgements of likeability. We tested five target trajectories of an AV in curves: playback manual driving, two stereotypical automated driving conditions (road centre tendency, lane centre tendency), and two stereotypical manual driving conditions, which slowed down for curves and cut curves. In addition, four braking patterns for approaching a zebra crossing were tested: manual braking, stereotypical automated driving (fixed deceleration), and two variations of stereotypical manual driving (sudden stop, crawling forward). The AV was observed by 24 participants standing on the curb of the road in groups. After each passing of the AV, participants rated whether the car was driven manually or automatically, and the degree to which they liked the AV’s behaviour. Results showed that the playback manual trajectory was considered more manual than the other trajectory conditions. The stereotype automated ‘road centre tendency’ and ‘lane centre tendency’ trajectories received similar likeability ratings as the playback manual driving. An analysis of written comments showed that curve cutting was a reason to believe the car is driving manually, whereas driving at a constant speed or in the centre was associated with automated driving. The sudden stop was the least likeable way to decelerate, but there was no consensus on whether this behaviour was manual or automated. It is concluded that AVs do not have to drive like a human in order to be liked."

# - title: "How should external Human-Machine Interfaces behave? Examining the effects of colour, position, message, activation distance, vehicle yielding, and visual distraction among 1,434 participants"
#   authors: Bazilinskyy, P., Kooijman, L., Dodou, D., De Winter, J. C. F.
#   url: bazilinskyy2021should
#   image: bazilinskyy2021should.jpg
#   display: Applied Ergonomics, 95, 103450
#   year: 2021
#   doi: 10.1016/j.apergo.2021.103450
#   suppmat: https://doi.org/10.4121/14465715
#   abstract: "External human-machine interfaces (eHMIs) may be useful for communicating the intention of an automated vehicle (AV) to a pedestrian, but it is unclear which eHMI design is most effective. In a crowdsourced experiment, we examined the effects of (1) colour (red, green, cyan), (2) position (roof, bumper, windshield), (3) message (WALK, DON'T WALK, WILL STOP, WON'T STOP, light bar), (4) activation distance (35 or 50 m from the pedestrian), and (5) the presence of visual distraction in the environment, on pedestrians' perceived safety of crossing the road in front of yielding and non-yielding AVs. Participants (N = 1434) had to press a key when they felt safe to cross while watching a random 40 out of 276 videos of an approaching AV with eHMI. Results showed that (1) green and cyan eHMIs led to higher perceived safety of crossing than red eHMIs; no significant difference was found between green and cyan, (2) eHMIs on the bumper and roof were more effective than eHMIs on the windshield, (3) for yielding AVs, perceived safety was higher for WALK compared to WILL STOP, followed by the light bar; for non-yielding AVs, a red bar yielded similar results to red text, (4) for yielding AVs, a red bar caused lower perceived safety when activated early compared to late, whereas green/cyan WALK led to higher perceived safety when activated late compared to early, and (5) distraction had no significant effect. We conclude that people adopt an egocentric perspective, that the windshield is an ineffective position, that the often-recommended colour cyan may have to be avoided, and that eHMI activation distance has intricate effects related to onset saliency."

# - title: "Towards the detection of driver–pedestrian eye contact"
#   authors: Onkhar, V., Bazilinskyy, P., Stapel, J. C. J., Dodou, D., Gavrila, D., De Winter, J. C. F.
#   url: onkhar2021towards
#   image: onkhar2021towards.jpg
#   display: Pervasive and Mobile Computing, 76, 101455
#   year: 2021
#   doi: 10.1016/j.pmcj.2021.101455
#   suppmat: https://doi.org/10.4121/15134037
#   abstract: "Non-verbal communication, such as eye contact between drivers and pedestrians, has been regarded as one way to reduce accident risk. So far, studies have assumed rather than objectively measured the occurrence of eye contact. We address this research gap by developing an eye contact detection method and testing it in an indoor experiment with scripted driver-pedestrian interactions at a pedestrian crossing. Thirty participants acted as a pedestrian either standing on an imaginary curb or crossing an imaginary one-lane road in front of a stationary vehicle with an experimenter in the driver’s seat. In half of the trials, pedestrians were instructed to make eye contact with the driver; in the other half, they were prohibited from doing so. Both parties’ gaze was recorded using eye trackers. An in-vehicle stereo camera recorded the car’s point of view, a head-mounted camera recorded the pedestrian’s point of view, and the location of the driver’s and pedestrian’s eyes was estimated using image recognition. We demonstrate that eye contact can be detected by measuring the angles between the vector joining the estimated location of the driver’s and pedestrian’s eyes, and the pedestrian’s and driver’s instantaneous gaze directions, respectively, and identifying whether these angles fall below a threshold of 4°. We achieved 100% correct classification of the trials involving eye contact and those without eye contact, based on measured eye contact duration. The proposed eye contact detection method may be useful for future research into eye contact."

# - title: "How do pedestrians distribute their visual attention when walking through a parking garage? An eye-tracking study."
#   authors: De Winter, J. C. F., Bazilinskyy, P., Wesdorp, D., De Vlam, V., Hopmans, B., Visscher, J., Dodou, D.
#   url: dewinter2021pedestrians
#   image: dewinter2021pedestrians.jpg
#   display: Ergonomics, 64, 793–805
#   year: 2021
#   doi: 10.1080/00140139.2020.1862310
#   suppmat: https://doi.org/10.4121/13488384.v1
#   abstract: "We examined what pedestrians look at when walking through a parking garage. Thirty-six participants walked a short route in a floor of a parking garage while their eye movements and head rotations were recorded with a Tobii Pro Glasses 2 eye-tracker. The participants’ fixations were then classified into 14 areas of interest. The results showed that pedestrians often looked at the back (20.0%), side (7.5%), and front (4.2%) of parked cars, and at approaching cars (8.8%). Much attention was also paid to the ground (20.1%). The wheels of cars (6.8%) and the driver in approaching cars (3.2%) received attention as well. In conclusion, this study showed that eye movements are largely functional in the sense that they appear to assist in safe navigation through the parking garage. Pedestrians look at a variety of sides and features of the car, suggesting that displays on future automated cars should be omnidirectionally visible. Practioner summary: This study measured where pedestrians look when walking through a parking garage. It was found that the back, side, and wheels of cars attract considerable attention. This knowledge may be important for the development of automated cars that feature so-called external human-machine interfaces (eHMIs)."

# - title: "Automated vehicles that communicate implicitly: Examining the use of lateral position within the lane"
#   authors: Sripada, A., Bazilinskyy, P., De Winter, J. C. F.
#   url: sripada2021automated
#   image: sripada2021automated.jpg
#   display: Ergonomics, 1–13
#   year: 2021
#   doi: 10.1080/00140139.2021.1925353
#   suppmat: https://doi.org/10.4121/14046107
#   abstract: "It may be necessary to introduce new modes of communication between automated vehicles (AVs) and pedestrians. This research proposes using the AV’s lateral deviation within the lane to communicate if the AV will yield to the pedestrian. In an online experiment, animated video clips depicting an approaching AV were shown to participants. Each of 1104 participants viewed 28 videos twice in random order. The videos differed in deviation magnitude, deviation onset, turn indicator usage, and deviation-yielding mapping. Participants had to press and hold a key as long as they felt safe to cross, and report the perceived intuitiveness of the AV’s behaviour after each trial. The results showed that the AV moving towards the pedestrian to indicate yielding and away to indicate continuing driving was more effective than the opposite combination. Furthermore, the turn indicator was regarded as intuitive for signalling that the AV will yield. Practitioner summary: Future automated vehicles (AVs) may have to communicate with vulnerable road users. Many researchers have explored explicit communication via text messages and led strips on the outside of the AV. The present study examines the viability of implicit communication via the lateral movement of the AV."

# - title: "Visual attention of pedestrians in traffic scenes: A crowdsourcing experiment"
#   authors: Bazilinskyy, P., Kyriakidis, M., De Winter, J. C. F.
#   url: bazilinskyy2021visual
#   image: bazilinskyy2021visual.jpg
#   display: Proceedings of International Conference on Applied Human Factors and Ergonomics (AHFE). New York, USA
#   year: 2021
#   suppmat: https://doi.org/10.4121/13614824
#   code: https://github.com/bazilinskyy/gazes-crowdsourced
#   abstract: "In a crowdsourced experiment, the effects of distance and type of the approaching vehicle, traffic density, and visual clutter on pedestrians’ attention distribution were explored. 965 participants viewed 107 images of diverse traffic scenes for durations between 100 and 4000 ms. Participants’ eye-gaze data were collected using the TurkEyes method. The method involved briefly showing codecharts after each image and asking the participants to type the code they saw last. The results indicate that automated vehicles were more often glanced at than manual vehicles. Measuring eye gaze without an eye tracker is promising."

# - title: "Bio-inspired intent communication for automated vehicles"
#   authors: Oudshoorn, M. P. J., De Winter, J. C. F., Bazilinskyy, P., Dodou, D.
#   url: oudshoorn2021bio
#   image: oudshoorn2021bio.jpg
#   display: "Transportation Research Part F: Traffic Psychology and Behaviour, 80, 127-140"
#   year: 2021
#   doi: 10.1016/j.trf.2021.03.021
#   suppmat: https://doi.org/10.4121/14096067
#   abstract: "Various external human-machine interfaces (eHMIs) have been proposed that communicate the intent of automated vehicles (AVs) to vulnerable road users. However, there is no consensus on which eHMI concept is most suitable for intent communication. In nature, animals have evolved the ability to communicate intent via visual signals. Inspired by intent communication in nature, this paper investigated three novel and potentially intuitive eHMI designs that rely on posture, gesture, and colouration, respectively. In an online crowdsourcing study, 1141 participants viewed videos featuring a yielding or non-yielding AV with one of the three bio-inspired eHMIs, as well as a green/red lightbar eHMI, a walk/-don't walk text-based eHMI, and a baseline condition (i.e., no eHMI). Participants were asked to press and hold a key when they felt safe to cross and to answer rating questions. Together, these measures were used to determine the intuitiveness of the tested eHMIs. Results showed that the lightbar eHMI and text-based eHMI were more intuitive than the three bio-inspired eHMIs, which, in turn, were more intuitive than the baseline condition. An exception was the bio-inspired colouration eHMI, which produced a performance score that was equivalent to the text-based eHMI when communicating 'non-yielding'. Further research is necessary to examine whether these observations hold in more complex traffic situations. Additionally, we recommend combining features from different eHMIs, such as the full-body communication of the bio-inspired colouration eHMI with the colours of the lightbar eHMI."

# - title: "The effect of drivers’ eye contact on pedestrians’ perceived safety"
#   authors: Onkhar, V., Bazilinskyy, P., Dodou, D., De Winter, J. C. F.
#   url: onkhar2022effect
#   image: onkhar2022effect.jpg
#   display: "Transportation Research Part F: Traffic Psychology and Behaviour, 84, 194-210"
#   year: 2022
#   doi: 10.1016/j.trf.2021.10.017
#   suppmat: https://doi.org/10.4121/16866709
#   abstract: "Many fatal accidents that involve pedestrians occur at road crossings, and are attributed to a breakdown of communication between pedestrians and drivers. Thus, it is important to investigate how forms of communication in traffic, such as eye contact, influence crossing decisions. Thus far, there is little information about the effect of drivers’ eye contact on pedestrians’ perceived safety to cross the road. Existing studies treat eye contact as immutable, i.e., it is either present or absent in the whole interaction, an approach that overlooks the effect of the timing of eye contact. We present an online crowdsourced study that addresses this research gap. 1835 participants viewed 13 videos of an approaching car twice, in random order, and held a key whenever they felt safe to cross. The videos differed in terms of whether the car yielded or not, whether the car driver made eye contact or not, and the times when the driver made eye contact. Participants also answered questions about their perceived intuitiveness of the driver’s eye contact behavior. The results showed that eye contact made people feel considerably safer to cross compared to no eye contact (an increase in keypress percentage from 31% to 50% was observed). In addition, the initiation and termination of eye contact affected perceived safety to cross more strongly than continuous eye contact and a lack of it, respectively. The car’s motion, however, was a more dominant factor. Additionally, the driver’s eye contact when the car braked was considered intuitive, and when it drove off, counterintuitive. In summary, this study demonstrates for the first time how drivers’ eye contact affects pedestrians’ perceived safety as a function of time in a dynamic scenario and questions the notion in recent literature that eye contact in road interactions is dispensable. These findings may be of interest in the development of automated vehicles (AVs), where the driver of the AV might not always be paying attention to the environment."

# - title: "Crowdsourced assessment of 227 text-based eHMIs for a crossing scenario"
#   authors: Bazilinskyy, P., Dodou, D., De Winter, J. C. F.
#   url: bazilinskyy2022crowdsourced
#   image: bazilinskyy2022crowdsourced.jpg
#   display: Proceedings of International Conference on Applied Human Factors and Ergonomics (AHFE). New York, USA
#   year: 2022
#   code: https://github.com/bazilinskyy/text-crowdsourced
#   suppmat: https://doi.org/10.4121/19102133
#   abstract: "Future automated vehicles may be equipped with external human-machine interfaces (eHMIs) capable of signaling whether pedestrians can cross the road. Industry and academia have proposed a variety of eHMIs featuring a text message. An eHMI message can refer to the action to be performed by the pedestrian (egocentric message) or the automated vehicle (allocentric message). Currently, there is no consensus on the correct phrasing of the text message. We created 227 eHMIs based on text-based eHMIs observed in the literature. A crowdsourcing experiment (N = 1241) was performed with images depicting an automated vehicle equipped with an eHMI on the front bumper. The participants indicated whether they would (not) cross the road, and response times were recorded. Egocentric messages were found to be more compelling for participants to (not) cross than allocentric messages. Furthermore, Spanish-speaking participants found Spanish eHMIs more compelling than English eHMIs. Finally, it was established that some eHMI texts should be avoided, as signified by compellingness, long responses, and high inter-subject variability."

# - title: "Identifying lane changes automatically using the GPS sensors of portable devices"
#   authors: Driessen, T., Prasad, L., Bazilinskyy, P., De Winter, J. C. F.
#   url: driessen2022identifying
#   image: driessen2022identifying.jpg
#   display: Proceedings of International Conference on Applied Human Factors and Ergonomics (AHFE). New York, USA
#   year: 2022
#   code: https://github.com/tomdries/gps-lane-changes
#   suppmat: https://doi.org/10.4121/19170302
#   abstract: "Mobile applications that provide GPS-based route navigation advice or driver diagnostics are gaining popularity. However, these applications currently do not have knowledge of whether the driver is performing a lane change. Having such information may prove valuable to individual drivers (e.g., to provide more specific navigation instructions) or road authorities (e.g., knowledge of lane change hotspots may inform road design). The present study aimed to assess the accuracy of lane change recognition algorithms that rely solely on mobile GPS sensor input. Three trips on Dutch highways, totaling 158 km of driving, were performed while carrying two smartphones (Huawei P20, Samsung Galaxy S9), a GPS-equipped GoPro Max, and a USB GPS receiver (GlobalSat BU343-s4). The timestamps of all 215 lane changes were manually extracted from the forward-facing GoPro camera footage, and used as ground truth. After connecting the GPS trajectories to the road using Mapbox Map Matching API (2022), lane changes were identified based on the exceedance of a lateral translation threshold in set time windows. Different thresholds and window sizes were tested for their ability to discriminate between a pool of lane change segments and an equally-sized pool of no-lane-change segments. The overall accuracy of the lane-change classification was found to be 90%. The method appears promising for highway engineering and traffic behavior research that use floating car data, but there may be limited applicability to real-time advisory systems due to the occasional occurrence of false positives."

# - title: "Stopping by looking: A driver-pedestrian interaction study in a coupled simulator using head-mounted displays with eye-tracking"
#   authors: Mok, C. S., Bazilinskyy, P., De Winter, J. C. F.
#   url: mok2022stopping
#   image: mok2022stopping.jpg
#   display: Applied Ergonomics, 105, 103825
#   suppmat: https://www.sciencedirect.com/science/article/pii/S000368702200148X#appsec1
#   year: 2022
#   doi: 10.1016/j.apergo.2022.103825
#   abstract: "Automated vehicles (AVs) can perform low-level control tasks but are not always capable of proper decision-making. This paper presents a concept of eye-based maneuver control for AV-pedestrian interaction. Previously, it was unknown whether the AV should conduct a stopping maneuver when the driver looks at the pedestrian or looks away from the pedestrian. A two-agent experiment was conducted using two head-mounted displays with integrated eye-tracking. Seventeen pairs of participants (pedestrian and driver) each interacted in a road crossing scenario. The pedestrians' task was to hold a button when they felt safe to cross the road, and the drivers' task was to direct their gaze according to instructions. Participants completed three 16-trial blocks: (1) Baseline, in which the AV was pre-programmed to yield or not yield, (2) Look to Yield (LTY), in which the AV yielded when the driver looked at the pedestrian, and (3) Look Away to Yield (LATY), in which the AV yielded when the driver did not look at the pedestrian. The driver's eye movements in the LTY and LATY conditions were visualized using a virtual light beam. Crossing performance was assessed based on whether the pedestrian held the button when the AV yielded and released the button when the AV did not yield. Furthermore, the pedestrians' and drivers' acceptance of the mappings was measured through a questionnaire. The results showed that the LTY and LATY mappings yielded better crossing performance than Baseline. Furthermore, the LTY condition was best accepted by drivers and pedestrians. Eye-tracking analyses indicated that the LTY and LATY mappings attracted the pedestrian's attention, while pedestrians still distributed their attention between the AV and a second vehicle approaching from the other direction. In conclusion, LTY control may be a promising means of AV control at intersections before full automation is technologically feasible."

# - title: "Get out of the way! Examining eHMIs in critical driver-pedestrian encounters in a coupled simulator"
#   authors: Bazilinskyy, P., Kooijman, L., Dodou, D., Mallant, K. P. T., Roosens, V. E. R., Middelweerd, M. D. L. M., Overbeek, L. D., De Winter, J. C. F.
#   url: bazilinskyy2022get
#   image: bazilinskyy2022get.jpg
#   display: Proceedings of International ACM Conference on Automotive User Interfaces and Interactive Vehicular Applications (AutoUI)
#   year: 2022
#   doi: 
#   code: https://github.com/bazilinskyy/coupled-sim-evasive
#   suppmat: https://doi.org/10.4121/20224281
#   abstract: "Past research suggests that displays on the exterior of the car, known as eHMIs, can be effective in helping pedestrians to make safe crossing decisions. This study examines a new application of eHMIs, namely the provision of directional information in scenarios where the pedestrian is almost hit by a car. In an experiment using a head-mounted display and a motion suit, participants had to cross the road while a car driven by another participant approached them. The results showed that the directional eHMI caused pedestrians to step back compared to no eHMI. The eHMI increased the pedestrians’ self-reported understanding of the car’s intention, although some pedestrians did not notice the eHMI. In conclusion, there may be potential for supporting pedestrians in situations where they need support the most, namely critical encounters. Future research may consider coupling a directional eHMI to autonomous emergency steering."

# - title: "Blinded windows and empty driver seats: The effects of automated vehicle characteristics on cyclist decision-making"
#   authors: Bazilinskyy, P., Dodou, D., Eisma, Y. B., Vlakveld, W. V., De Winter, J. C. F.
#   url: bazilinskyy2022blinded
#   image: bazilinskyy2022blinded.jpg
#   display: IET Intelligent Transportation Systems
#   year: 2022
#   doi: 10.1049/itr2.12235
#   suppmat: https://doi.org/10.4121/20103188
#   abstract: "Automated vehicles (AVs) may feature blinded (i.e., blacked-out) windows and external Human-Machine Interfaces (eHMIs), and the driver may be inattentive or absent, but how these features affect cyclists is unknown. In a crowdsourcing study, participants viewed images of approaching vehicles from a cyclist’s perspective and decided whether to brake. The images depicted different combinations of traditional versus automated vehicles, eHMI presence, vehicle approach direction, driver visibility/window-blinding, visual complexity of the surroundings, and distance to the cyclist (urgency). The results showed that the eHMI and urgency level had a strong impact on crossing decisions, whereas visual complexity had no significant influence. Blinded windows caused participants to brake for the traditional vehicle. A second crowdsourcing experiment aimed to clarify the findings of Experiment 1 by also requiring participants to detect the vehicle features. It was found that the eHMI ‘GO’ and blinded windows yielded high detection rates and that driver eye contact caused participants to continue pedalling. To conclude, blinded windows increase the probability that cyclists brake, and driver eye contact stimulates cyclists to continue cycling. Our findings, which were obtained with large international samples, may help elucidate how AVs (in which the driver may not be visible) affect cyclists’ behaviour."

# - title: "Intent communication in nature: An overview of biological paradigms and their applicability to automated vehicle"
#   authors: Oudshoorn, M. P. J., De Winter, J. C. F., Bazilinskyy, P., Dodou, D.
#   url: oudshoorn2021intent
#   image: oudshoorn2021intent.jpg
#   display: Manuscript in preparation
#   year:
#   abstract: "With the advent of automated vehicles (AVs), several means of visual communication that are currently used in interactions between a human driver and pedestrians (e.g., eye contact, hand gestures) may no longer be available. External human-machine interfaces (eHMIs) can serve to communicate intent from the AV to pedestrians. There are numerous methods of intent-communication methods in nature which, due to their long evolution, might provide inspiration for the design of effective and intuitive eHMIs. This paper reviews visual intent-communication methods in nature and investigates their applicability for eHMIs. We found that posture, gesture, facial expression, colouration, and bioluminescence are common intent-communication methods used in nature for a variety of functions, including agonistic interactions, courtship rituals, alarm calls, and food acquisition. Gestures and colouration appear to be used for all function types, and posture and facial expression are more often employed in agonistic interactions (i.e., social behaviours related to fighting). Signals were often dynamic, which increases their detectability. The simultaneous use of multiple channels was often encountered. Inspired by the identified communication methods in nature, we provided suggestions about how these methods can be translated to eHMI design concepts, including changing the size of the vehicle, using foldable structures, and applying conspicuous colouration to the body of the AV."

# - title: "Synthetic sounds for automated vehicles: Loud is effective"
#   authors: Bazilinskyy, P., Merino-Martınez, R., Vieirac, E. O., Dodou, D., De Winter, J. C. F.
#   url: bazilinskyy2023synthetic
#   image: bazilinskyy2023synthetic.jpg
#   display: <b>Manuscript submitted for publication</b>
#   year:
#   abstract: "Synthetic vehicle sounds have been introduced in electric vehicles and as external human-machine interfaces for automated vehicles. While previous research has mostly studied the effect of synthetic vehicle sounds on detectability and acceptance, the present study takes a different approach by examining the efficacy of synthetic vehicle sounds in refraining people from crossing the road. An online study was conducted in which 125 participants were presented with different types of synthetic sounds, including sounds of a combustion engine, pure tones, combined tones, and beeps. A straight vehicle trajectory with a constant velocity of 30 km/h was used, and no visual information was provided. Participants in the role of pedestrians were asked to hold a key as long as they felt safe to cross. Additionally, after each trial, participants were asked to assess whether the vehicle sound was easy to notice, whether it gave enough information to realize a vehicle was approaching, and whether the sound was annoying. The results showed that louder sounds were perceived as more annoying, but were also the most effective in preventing participants from crossing the road. This finding raises the question of how to design vehicle sounds that are noticeable and compelling, yet not too annoying."

# - title: "Predicting perceived risk of traffic scenes using computer vision"
#   authors: De Winter, J. C. F., Hoogmoed, J., Stapel, J., Dodou, D., Bazilinskyy, P.
#   url: dewinter2022predicting
#   image: dewinter2022predicting.jpg
#   display: <b>Manuscript submitted for publication</b>
#   year:
#   abstract: "Perceived (i.e., subjective) risk is a key construct in traffic psychology and, more recently, in the area of automated driving. If the automated vehicle were able to predict the occupants’ perceived risk, this would allow the automated vehicle to adjust its behavior accordingly. This paper examines whether perceived risk in images of traffic scenes is predictable from computer-vision features that may also be used by AVs. A total of 1378 participants rated the perceived risk for a random 100 of 210 images of traffic scenes from the KITTI dataset. The population-level perceived risk was found to be statistically reliable (r = 0.98 split-half reliability). Linear regression showed that perceived risk was predictable (r = 0.63) from two features obtained with the YOLOv4 computer-vision algorithm: the number of vulnerable road users in the scene and the mean size of the bounding boxes surrounding other road users. In a second regression model, the ego-vehicle’s speed was added as an independent variable, which substantially increased the strength of the prediction to r = 0.75. Paradoxically, the sign of the speed prediction was negative; that is, a higher vehicle speed implied lower perceived risk, a finding that resonates with the principle of self-explaining roads. It is concluded that objective features can contribute to an accurate prediction of population subjective risk and substantially outperforms the ratings provided by individual participants (mean r = 0.41). Computer-vision methods may become increasingly relevant to understanding human factors constructs in the field of traffic psychology."

# - title: "The identification of factors affecting drivers’ perceived risk in pedestrian-vehicle interaction: A crowdsourcing study"
#   authors: Bazilinskyy, P., Kooijman, B., Dodou, D., De Winter, J. C. F.
#   url: bazilinskyy2022identification
#   image: bazilinskyy2022identification.jpg
#   display: Manuscript in preparation
#   year:
#   abstract: "Previous research showed that perceived risk is an important psychological determinant governing road user behaviour. However, little knowledge exists about how objective in-scene features affect a driver’s perceived risk in interactions with pedestrians. This crowdsourcing study tries to fill this research gap. A total of 1082 participants watched 35 out of a total of 86 dashcam videos featuring interactions with pedestrians extracted from the Pedestrian Intention Estimation (PIE) dataset. The videos contained annotations of pedestrian eye contact, crossing behaviour, GPS location, vehicle speed, and yielding rules. The distance between vehicle and pedestrian was manually added, and object counts (detected number of pedestrians, cyclists and vehicles) and respective sizes were added as an index of visual clutter. In each video, participants were asked to press a key on their keyboard and hold it as long as they felt a situation could become risky, and after each video rate perceived risk using a slider and answer whether the pedestrian had made eye contact. Videos in which the participant observed eye contact, increased perceived risk, suggesting that eye contact increases drivers’ vigilance. Videos with more visual clutter, and with higher vehicle speed were also associated with increased perceived risk. However, the causality of the correlation with vehicle speed can be questioned and may be mediated by the environment and whether crossing occurred. Videos in which yielding rules were absent, compared to videos in which they were present, did not affect perceived risk. This study is the first to investigate how pedestrians’ eye contact affects drivers’ perceived risk. The presented results could be useful in safe road design or be used as input for eHMI activation to enhance safety."

# - title: "Automated lane changing using deep reinforcement learning: A user-acceptance case study"
#   authors: Van der Haak, D., Bazilinskyy, P., Dodou, D., De Winter, J. C. F.
#   url: vanderhaak2022automated
#   image: vanderhaak2022automated.jpg
#   display: Manuscript in preparation
#   year:
#   abstract: "Lane change decision-making is an important challenge for automated vehicles, urging the need for high performance algorithms that are able to handle complex traffic situations. Deep reinforcement learning (DRL), a machine learning method based on artificial neural networks, has recently become a popular choice for modelling the lane change decision-making process, outperforming various traditional rule-based models. So far, performance has often been expressed in terms of achieved average speed, absence of collisions or merging success rate. However, no studies have investigated how humans will react to the resulting behavior as potential occupants. This study addresses this research gap by validating a self-developed DRL-based lane changing model (trained using proximal policy optimization) from a technology acceptance perspective through an online crowdsourcing experiment. Participants (N = 1085) viewed a random subset of 32 out of 120 videos of an automated vehicle driving on a three-lane highway with varying traffic densities featuring our proposed model or a baseline policy (i.e. a state-of-the-art rule-based model, MOBIL). They were tasked to press a response key if the decision-making was deemed undesirable and subsequently rated the vehicle’s behavior along four acceptance constructs (performance expectancy, safety, human-likeness and reliability) on a scale of 1 to 5. Results showed that the proposed model caused a significantly lower amount of disagreements and was rated significantly higher on all four acceptance constructs compared to the baseline policy. Moreover, considerable differences between individual disagreement rates were observed for both models. Our findings offer prospects for the practical application of DRL-based lane change models in a use-case scenario, depending on the user. Further research is necessary to examine whether these observations hold in other (more complex) traffic situations. Additionally, we recommend combining DRL with other modelling techniques that allow for personalization of behavioral parameters, such as imitation learning."


# # - title: ""
# #   authors:
# #     Bazilinskyy, P., ...
# #   url: bazilinskyy20xx...
# #   image: dummy.jpg
# #   display:
# #   year:
# #   doi:
# #   suppmat:
# #   abstract: ""